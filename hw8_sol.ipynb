{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 8: Time series\n",
    "**Due date: [April 10, 11:59pm](https://github.com/UBC-CS/cpsc330-2023W2?tab=readme-ov-file#deliverable-due-dates-tentative).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "<hr>\n",
    "\n",
    "_points: 2_\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. To submit this assignment, follow the instructions below:\n",
    "\n",
    "- **You may work on this assignment in a group (group size <= 4) and submit your assignment as a group.** \n",
    "- Below are some instructions on working as a group.  \n",
    "    - The maximum group size is 4. \n",
    "    - You can choose your own group members. \n",
    "    - Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "    - Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "    - It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. [Here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members) are some instructions on adding group members in Gradescope.  \n",
    "- Be sure to follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W2/blob/main/docs/homework_instructions.md).\n",
    "- Upload the .ipynb file to PrairieLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture, we had different measurements for each Location. How about this avocado sales dataset? For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_col = df[\"region\"].unique()\n",
    "len(unique_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely should separate by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1373.95</td>\n",
       "      <td>57.42</td>\n",
       "      <td>153.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>435021.49</td>\n",
       "      <td>364302.39</td>\n",
       "      <td>23821.16</td>\n",
       "      <td>82.15</td>\n",
       "      <td>46815.79</td>\n",
       "      <td>16707.15</td>\n",
       "      <td>30108.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3846.69</td>\n",
       "      <td>1500.15</td>\n",
       "      <td>938.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1408.19</td>\n",
       "      <td>1071.35</td>\n",
       "      <td>336.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>788025.06</td>\n",
       "      <td>53987.31</td>\n",
       "      <td>552906.04</td>\n",
       "      <td>39995.03</td>\n",
       "      <td>141136.68</td>\n",
       "      <td>137146.07</td>\n",
       "      <td>3990.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>BaltimoreWashington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume       4046       4225      4770  \\\n",
       "51 2015-01-04          1.22      40873.28    2819.50   28287.42     49.90   \n",
       "51 2015-01-04          1.79       1373.95      57.42     153.88      0.00   \n",
       "51 2015-01-04          1.00     435021.49  364302.39   23821.16     82.15   \n",
       "51 2015-01-04          1.76       3846.69    1500.15     938.35      0.00   \n",
       "51 2015-01-04          1.08     788025.06   53987.31  552906.04  39995.03   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "51     1162.65     1162.65        0.00          0.0       organic  2015   \n",
       "51    46815.79    16707.15    30108.64          0.0  conventional  2015   \n",
       "51     1408.19     1071.35      336.84          0.0       organic  2015   \n",
       "51   141136.68   137146.07     3990.61          0.0  conventional  2015   \n",
       "\n",
       "                 region  \n",
       "51               Albany  \n",
       "51               Albany  \n",
       "51              Atlanta  \n",
       "51              Atlanta  \n",
       "51  BaltimoreWashington  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"Date\", \"region\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same `region`, we could have 2 rows due to different `type`, so we should separate by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "51 2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "50 2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "49 2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "48 2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "47 2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "50     8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "49    11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "48    10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "47     9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "\n",
       "    region  \n",
       "51  Albany  \n",
       "50  Albany  \n",
       "49  Albany  \n",
       "48  Albany  \n",
       "47  Albany  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"region\", \"type\", \"Date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have separate measurements per day for 54 different regions as well as the different type of avocados (grown conventionally or organically). So we want to separate the values by `region` and `type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements appear to be very equally spaced in all regions and all farming types, exactly 7 days. There appears to be three missing values in the \"WestTexNewMexico\" region for organically grown avocados, since there is one spaced 14 weeks and one space 21 weeks apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Albany', 'conventional')     Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Albany', 'organic')          Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Atlanta', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Atlanta', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('BaltimoreWashington', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('BaltimoreWashington', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Boise', 'conventional')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Boise', 'organic')           Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Boston', 'conventional')     Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Boston', 'organic')          Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('BuffaloRochester', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('BuffaloRochester', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('California', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('California', 'organic')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Charlotte', 'conventional')  Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Charlotte', 'organic')       Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Chicago', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Chicago', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('CincinnatiDayton', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('CincinnatiDayton', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Columbus', 'conventional')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Columbus', 'organic')        Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('DallasFtWorth', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('DallasFtWorth', 'organic')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Denver', 'conventional')     Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Denver', 'organic')          Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Detroit', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Detroit', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('GrandRapids', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('GrandRapids', 'organic')     Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('GreatLakes', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('GreatLakes', 'organic')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('HarrisburgScranton', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('HarrisburgScranton', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('HartfordSpringfield', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('HartfordSpringfield', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Houston', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Houston', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Indianapolis', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Indianapolis', 'organic')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Jacksonville', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Jacksonville', 'organic')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('LasVegas', 'conventional')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('LasVegas', 'organic')        Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('LosAngeles', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('LosAngeles', 'organic')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Louisville', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Louisville', 'organic')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('MiamiFtLauderdale', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('MiamiFtLauderdale', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Midsouth', 'conventional')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Midsouth', 'organic')        Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Nashville', 'conventional')  Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Nashville', 'organic')       Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('NewOrleansMobile', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('NewOrleansMobile', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('NewYork', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('NewYork', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Northeast', 'conventional')  Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Northeast', 'organic')       Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('NorthernNewEngland', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('NorthernNewEngland', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Orlando', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Orlando', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Philadelphia', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Philadelphia', 'organic')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('PhoenixTucson', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('PhoenixTucson', 'organic')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Pittsburgh', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Pittsburgh', 'organic')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Plains', 'conventional')     Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Plains', 'organic')          Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Portland', 'conventional')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Portland', 'organic')        Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('RaleighGreensboro', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('RaleighGreensboro', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('RichmondNorfolk', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('RichmondNorfolk', 'organic') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Roanoke', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Roanoke', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Sacramento', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Sacramento', 'organic')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SanDiego', 'conventional')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SanDiego', 'organic')        Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SanFrancisco', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SanFrancisco', 'organic')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Seattle', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Seattle', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SouthCarolina', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SouthCarolina', 'organic')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SouthCentral', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('SouthCentral', 'organic')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Southeast', 'conventional')  Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Southeast', 'organic')       Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Spokane', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Spokane', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('StLouis', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('StLouis', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Syracuse', 'conventional')   Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Syracuse', 'organic')        Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Tampa', 'conventional')      Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('Tampa', 'organic')           Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('TotalUS', 'conventional')    Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('TotalUS', 'organic')         Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('West', 'conventional')       Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('West', 'organic')            Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('WestTexNewMexico', 'conventional') Date\n",
      "7 days    142\n",
      "Name: count, dtype: int64\n",
      "('WestTexNewMexico', 'organic') Date\n",
      "7 days     137\n",
      "14 days      1\n",
      "21 days      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for name, group in df_train.groupby(['region', \"type\"]):\n",
    "    print(\"%-30s %s\" % (name, group[\"Date\"].sort_values().diff().value_counts()))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WestTexNewMexico', 'organic')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be quite a bit of overlapping regions.\n",
    "\n",
    "TotalUS overlaps with all the other regions.\n",
    "Sacramento, San Francisco, San Diego, Los Angeles all overlaps with California, which overlaps with the \"West\", which overlaps with TotalUS.\n",
    "\n",
    "Northeast, Northern New England, not sure what specific locations they mean but maybe they overlap.\n",
    "\n",
    "Not sure if NewYork is referencing the city or the state. It may overlap with Northeast, and maybe Albany as well.\n",
    "\n",
    "Confused by what \"Plains\" means, doea it imply Midwest or Midsouth?\n",
    "\n",
    "The regions in this case are not distinct at all, unless big regions exclude the smaller regions in their data which makes little sense to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston',\n",
       "       'BuffaloRochester', 'California', 'Charlotte', 'Chicago',\n",
       "       'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver',\n",
       "       'Detroit', 'GrandRapids', 'GreatLakes', 'HarrisburgScranton',\n",
       "       'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville',\n",
       "       'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale',\n",
       "       'Midsouth', 'Nashville', 'NewOrleansMobile', 'NewYork',\n",
       "       'Northeast', 'NorthernNewEngland', 'Orlando', 'Philadelphia',\n",
       "       'PhoenixTucson', 'Pittsburgh', 'Plains', 'Portland',\n",
       "       'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Sacramento',\n",
       "       'SanDiego', 'SanFrancisco', 'Seattle', 'SouthCarolina',\n",
       "       'SouthCentral', 'Southeast', 'Spokane', 'StLouis', 'Syracuse',\n",
       "       'Tampa', 'TotalUS', 'West', 'WestTexNewMexico'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.region.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from [Lecture 19](https://github.com/UBC-CS/cpsc330-2023W2/blob/main/lectures/mathias/19_time-series.ipynb), with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag, groupby, new_feature_name=None, clip=False):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "    \n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "    \n",
    "    new_df = df.assign(**{new_feature_name : np.nan})\n",
    "    for name, group in new_df.groupby(groupby):        \n",
    "        if lag < 0: # take values from the past\n",
    "            new_df.loc[group.index[-lag:],new_feature_name] = group.iloc[:lag][orig_feature].values\n",
    "        else:       # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][orig_feature].values\n",
    "            \n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "18248 2018-03-25          1.62      15303.40  2325.30   2171.66    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "18248    10806.44    10569.80      236.64          0.0       organic  2018   \n",
       "\n",
       "                 region  \n",
       "0                Albany  \n",
       "1                Albany  \n",
       "2                Albany  \n",
       "3                Albany  \n",
       "4                Albany  \n",
       "...                 ...  \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18243 2018-02-18          1.56      17597.12  1892.05   1928.36    0.00   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18243    13776.71    13553.53      223.18          0.0       organic  2018   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0                Albany                  1.24  \n",
       "1                Albany                  1.17  \n",
       "2                Albany                  1.06  \n",
       "3                Albany                  0.99  \n",
       "4                Albany                  0.99  \n",
       "...                 ...                   ...  \n",
       "18243  WestTexNewMexico                  1.57  \n",
       "18244  WestTexNewMexico                  1.54  \n",
       "18245  WestTexNewMexico                  1.56  \n",
       "18246  WestTexNewMexico                  1.56  \n",
       "18247  WestTexNewMexico                  1.62  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `AveragePrice` baseline \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ for trainning is 0.82858 and for testing is 0.76318."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285800937261841"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_r2 = r2_score(y_true = df_train[\"AveragePriceNextWeek\"], \n",
    "                    y_pred = df_train[\"AveragePrice\"])\n",
    "train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631780188583048"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_r2 = r2_score(y_true = df_test[\"AveragePriceNextWeek\"], \n",
    "                   y_pred = df_test[\"AveragePrice\"])\n",
    "\n",
    "test_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "\n",
    "_Points:_ 10\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. We got to 0.80, but not beyond that. Let us know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15441 entries, 0 to 18222\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   Date                  15441 non-null  datetime64[ns]\n",
      " 1   AveragePrice          15441 non-null  float64       \n",
      " 2   Total Volume          15441 non-null  float64       \n",
      " 3   4046                  15441 non-null  float64       \n",
      " 4   4225                  15441 non-null  float64       \n",
      " 5   4770                  15441 non-null  float64       \n",
      " 6   Total Bags            15441 non-null  float64       \n",
      " 7   Small Bags            15441 non-null  float64       \n",
      " 8   Large Bags            15441 non-null  float64       \n",
      " 9   XLarge Bags           15441 non-null  float64       \n",
      " 10  type                  15441 non-null  object        \n",
      " 11  year                  15441 non-null  int64         \n",
      " 12  region                15441 non-null  object        \n",
      " 13  AveragePriceNextWeek  15441 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(1), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value, no imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# lec 19\n",
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features + target)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "    )\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "    new_columns = numeric_features + ohe_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[\"AveragePriceNextWeek\"]\n",
    "    y_test = test_df[\"AveragePriceNextWeek\"]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any Date information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_train\n",
    "test_df = df_test\n",
    "\n",
    "numeric_features = [\"AveragePrice\", \"Total Volume\", \"4046\", \"4225\", \"4770\",\n",
    "                    \"Total Bags\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"]\n",
    "\n",
    "categorical_features = [\"type\", \"region\"]\n",
    "\n",
    "drop_features = [\"Date\", \"year\"]\n",
    "\n",
    "target = [\"AveragePriceNextWeek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15441 entries, 0 to 18222\n",
      "Data columns (total 65 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   AveragePrice                15441 non-null  float64\n",
      " 1   Total Volume                15441 non-null  float64\n",
      " 2   4046                        15441 non-null  float64\n",
      " 3   4225                        15441 non-null  float64\n",
      " 4   4770                        15441 non-null  float64\n",
      " 5   Total Bags                  15441 non-null  float64\n",
      " 6   Small Bags                  15441 non-null  float64\n",
      " 7   Large Bags                  15441 non-null  float64\n",
      " 8   XLarge Bags                 15441 non-null  float64\n",
      " 9   type_conventional           15441 non-null  float64\n",
      " 10  type_organic                15441 non-null  float64\n",
      " 11  region_Albany               15441 non-null  float64\n",
      " 12  region_Atlanta              15441 non-null  float64\n",
      " 13  region_BaltimoreWashington  15441 non-null  float64\n",
      " 14  region_Boise                15441 non-null  float64\n",
      " 15  region_Boston               15441 non-null  float64\n",
      " 16  region_BuffaloRochester     15441 non-null  float64\n",
      " 17  region_California           15441 non-null  float64\n",
      " 18  region_Charlotte            15441 non-null  float64\n",
      " 19  region_Chicago              15441 non-null  float64\n",
      " 20  region_CincinnatiDayton     15441 non-null  float64\n",
      " 21  region_Columbus             15441 non-null  float64\n",
      " 22  region_DallasFtWorth        15441 non-null  float64\n",
      " 23  region_Denver               15441 non-null  float64\n",
      " 24  region_Detroit              15441 non-null  float64\n",
      " 25  region_GrandRapids          15441 non-null  float64\n",
      " 26  region_GreatLakes           15441 non-null  float64\n",
      " 27  region_HarrisburgScranton   15441 non-null  float64\n",
      " 28  region_HartfordSpringfield  15441 non-null  float64\n",
      " 29  region_Houston              15441 non-null  float64\n",
      " 30  region_Indianapolis         15441 non-null  float64\n",
      " 31  region_Jacksonville         15441 non-null  float64\n",
      " 32  region_LasVegas             15441 non-null  float64\n",
      " 33  region_LosAngeles           15441 non-null  float64\n",
      " 34  region_Louisville           15441 non-null  float64\n",
      " 35  region_MiamiFtLauderdale    15441 non-null  float64\n",
      " 36  region_Midsouth             15441 non-null  float64\n",
      " 37  region_Nashville            15441 non-null  float64\n",
      " 38  region_NewOrleansMobile     15441 non-null  float64\n",
      " 39  region_NewYork              15441 non-null  float64\n",
      " 40  region_Northeast            15441 non-null  float64\n",
      " 41  region_NorthernNewEngland   15441 non-null  float64\n",
      " 42  region_Orlando              15441 non-null  float64\n",
      " 43  region_Philadelphia         15441 non-null  float64\n",
      " 44  region_PhoenixTucson        15441 non-null  float64\n",
      " 45  region_Pittsburgh           15441 non-null  float64\n",
      " 46  region_Plains               15441 non-null  float64\n",
      " 47  region_Portland             15441 non-null  float64\n",
      " 48  region_RaleighGreensboro    15441 non-null  float64\n",
      " 49  region_RichmondNorfolk      15441 non-null  float64\n",
      " 50  region_Roanoke              15441 non-null  float64\n",
      " 51  region_Sacramento           15441 non-null  float64\n",
      " 52  region_SanDiego             15441 non-null  float64\n",
      " 53  region_SanFrancisco         15441 non-null  float64\n",
      " 54  region_Seattle              15441 non-null  float64\n",
      " 55  region_SouthCarolina        15441 non-null  float64\n",
      " 56  region_SouthCentral         15441 non-null  float64\n",
      " 57  region_Southeast            15441 non-null  float64\n",
      " 58  region_Spokane              15441 non-null  float64\n",
      " 59  region_StLouis              15441 non-null  float64\n",
      " 60  region_Syracuse             15441 non-null  float64\n",
      " 61  region_Tampa                15441 non-null  float64\n",
      " 62  region_TotalUS              15441 non-null  float64\n",
      " 63  region_West                 15441 non-null  float64\n",
      " 64  region_WestTexNewMexico     15441 non-null  float64\n",
      "dtypes: float64(65)\n",
      "memory usage: 7.8 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")\n",
    "X_train_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.84\n",
      "Test score: 0.80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.332625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.084066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.080943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.063704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_organic</th>\n",
       "      <td>0.047173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.044899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.047173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.061248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.061967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.071683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.332625\n",
       "region_SanFrancisco         0.084066\n",
       "region_HartfordSpringfield  0.080943\n",
       "region_NewYork              0.063704\n",
       "type_organic                0.047173\n",
       "...                              ...\n",
       "region_Denver              -0.044899\n",
       "type_conventional          -0.047173\n",
       "region_SouthCentral        -0.061248\n",
       "region_DallasFtWorth       -0.061967\n",
       "region_Houston             -0.071683\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge\n",
    "# lec19\n",
    "\n",
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, Ridge(alpha=1)) # i tried different alphas but they didn't affect train/test scores\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"ridge\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)\n",
    "\n",
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_train.assign(\n",
    "    Month=df_train[\"Date\"].apply(lambda x: x.month_name())\n",
    ")  # x.month_name() to get the actual string\n",
    "test_df = df_test.assign(Month=df_test[\"Date\"].apply(lambda x: x.month_name()))\n",
    "\n",
    "train_df[[\"Date\", \"Month\"]].sort_values(by=\"Month\")\n",
    "\n",
    "numeric_features = [\"AveragePrice\", \"Total Volume\", \"4046\", \"4225\", \"4770\", \"year\",\n",
    "                    \"Total Bags\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"]\n",
    "\n",
    "categorical_features = [\"type\", \"region\", \"Month\"]\n",
    "\n",
    "drop_features = [\"Date\"]\n",
    "\n",
    "target = [\"AveragePriceNextWeek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15441 entries, 0 to 18222\n",
      "Data columns (total 78 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   AveragePrice                15441 non-null  float64\n",
      " 1   Total Volume                15441 non-null  float64\n",
      " 2   4046                        15441 non-null  float64\n",
      " 3   4225                        15441 non-null  float64\n",
      " 4   4770                        15441 non-null  float64\n",
      " 5   year                        15441 non-null  float64\n",
      " 6   Total Bags                  15441 non-null  float64\n",
      " 7   Small Bags                  15441 non-null  float64\n",
      " 8   Large Bags                  15441 non-null  float64\n",
      " 9   XLarge Bags                 15441 non-null  float64\n",
      " 10  type_conventional           15441 non-null  float64\n",
      " 11  type_organic                15441 non-null  float64\n",
      " 12  region_Albany               15441 non-null  float64\n",
      " 13  region_Atlanta              15441 non-null  float64\n",
      " 14  region_BaltimoreWashington  15441 non-null  float64\n",
      " 15  region_Boise                15441 non-null  float64\n",
      " 16  region_Boston               15441 non-null  float64\n",
      " 17  region_BuffaloRochester     15441 non-null  float64\n",
      " 18  region_California           15441 non-null  float64\n",
      " 19  region_Charlotte            15441 non-null  float64\n",
      " 20  region_Chicago              15441 non-null  float64\n",
      " 21  region_CincinnatiDayton     15441 non-null  float64\n",
      " 22  region_Columbus             15441 non-null  float64\n",
      " 23  region_DallasFtWorth        15441 non-null  float64\n",
      " 24  region_Denver               15441 non-null  float64\n",
      " 25  region_Detroit              15441 non-null  float64\n",
      " 26  region_GrandRapids          15441 non-null  float64\n",
      " 27  region_GreatLakes           15441 non-null  float64\n",
      " 28  region_HarrisburgScranton   15441 non-null  float64\n",
      " 29  region_HartfordSpringfield  15441 non-null  float64\n",
      " 30  region_Houston              15441 non-null  float64\n",
      " 31  region_Indianapolis         15441 non-null  float64\n",
      " 32  region_Jacksonville         15441 non-null  float64\n",
      " 33  region_LasVegas             15441 non-null  float64\n",
      " 34  region_LosAngeles           15441 non-null  float64\n",
      " 35  region_Louisville           15441 non-null  float64\n",
      " 36  region_MiamiFtLauderdale    15441 non-null  float64\n",
      " 37  region_Midsouth             15441 non-null  float64\n",
      " 38  region_Nashville            15441 non-null  float64\n",
      " 39  region_NewOrleansMobile     15441 non-null  float64\n",
      " 40  region_NewYork              15441 non-null  float64\n",
      " 41  region_Northeast            15441 non-null  float64\n",
      " 42  region_NorthernNewEngland   15441 non-null  float64\n",
      " 43  region_Orlando              15441 non-null  float64\n",
      " 44  region_Philadelphia         15441 non-null  float64\n",
      " 45  region_PhoenixTucson        15441 non-null  float64\n",
      " 46  region_Pittsburgh           15441 non-null  float64\n",
      " 47  region_Plains               15441 non-null  float64\n",
      " 48  region_Portland             15441 non-null  float64\n",
      " 49  region_RaleighGreensboro    15441 non-null  float64\n",
      " 50  region_RichmondNorfolk      15441 non-null  float64\n",
      " 51  region_Roanoke              15441 non-null  float64\n",
      " 52  region_Sacramento           15441 non-null  float64\n",
      " 53  region_SanDiego             15441 non-null  float64\n",
      " 54  region_SanFrancisco         15441 non-null  float64\n",
      " 55  region_Seattle              15441 non-null  float64\n",
      " 56  region_SouthCarolina        15441 non-null  float64\n",
      " 57  region_SouthCentral         15441 non-null  float64\n",
      " 58  region_Southeast            15441 non-null  float64\n",
      " 59  region_Spokane              15441 non-null  float64\n",
      " 60  region_StLouis              15441 non-null  float64\n",
      " 61  region_Syracuse             15441 non-null  float64\n",
      " 62  region_Tampa                15441 non-null  float64\n",
      " 63  region_TotalUS              15441 non-null  float64\n",
      " 64  region_West                 15441 non-null  float64\n",
      " 65  region_WestTexNewMexico     15441 non-null  float64\n",
      " 66  Month_April                 15441 non-null  float64\n",
      " 67  Month_August                15441 non-null  float64\n",
      " 68  Month_December              15441 non-null  float64\n",
      " 69  Month_February              15441 non-null  float64\n",
      " 70  Month_January               15441 non-null  float64\n",
      " 71  Month_July                  15441 non-null  float64\n",
      " 72  Month_June                  15441 non-null  float64\n",
      " 73  Month_March                 15441 non-null  float64\n",
      " 74  Month_May                   15441 non-null  float64\n",
      " 75  Month_November              15441 non-null  float64\n",
      " 76  Month_October               15441 non-null  float64\n",
      " 77  Month_September             15441 non-null  float64\n",
      "dtypes: float64(78)\n",
      "memory usage: 9.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")\n",
    "X_train_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.316203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.100836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.098581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.077590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Philadelphia</th>\n",
       "      <td>0.057388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.052920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.057247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.074909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.075811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.087081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.316203\n",
       "region_SanFrancisco         0.100836\n",
       "region_HartfordSpringfield  0.098581\n",
       "region_NewYork              0.077590\n",
       "region_Philadelphia         0.057388\n",
       "...                              ...\n",
       "region_Denver              -0.052920\n",
       "type_conventional          -0.057247\n",
       "region_SouthCentral        -0.074909\n",
       "region_DallasFtWorth       -0.075811\n",
       "region_Houston             -0.087081\n",
       "\n",
       "[78 rows x 1 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge\n",
    "# lec19\n",
    "\n",
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, Ridge(alpha=1)) # i tried different alphas but they didn't affect train/test scores\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"ridge\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)\n",
    "\n",
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding with Month + Date since beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df[\"Date\"].min()\n",
    "train_df = df_train.assign(\n",
    "    DateSince=df_train[\"Date\"].apply(lambda x: (x - start_date).days),\n",
    "    Month=df_train[\"Date\"].apply(lambda x: x.month_name())\n",
    ")  # x.month_name() to get the actual string\n",
    "test_df = df_test.assign(\n",
    "    DateSince=df_test[\"Date\"].apply(lambda x: (x - start_date).days),\n",
    "    Month=df_train[\"Date\"].apply(lambda x: x.month_name())\n",
    ")\n",
    "\n",
    "train_df[[\"Date\", \"Month\", \"DateSince\"]].sort_values(by=\"Month\")\n",
    "\n",
    "numeric_features = [\"AveragePrice\", \"Total Volume\", \"4046\", \"4225\", \"4770\", \"year\", \n",
    "                    \"DateSince\", \"Total Bags\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"]\n",
    "\n",
    "categorical_features = [\"type\", \"region\", \"Month\"]\n",
    "\n",
    "drop_features = [\"Date\"]\n",
    "\n",
    "target = [\"AveragePriceNextWeek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15441 entries, 0 to 18222\n",
      "Data columns (total 79 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   AveragePrice                15441 non-null  float64\n",
      " 1   Total Volume                15441 non-null  float64\n",
      " 2   4046                        15441 non-null  float64\n",
      " 3   4225                        15441 non-null  float64\n",
      " 4   4770                        15441 non-null  float64\n",
      " 5   year                        15441 non-null  float64\n",
      " 6   DateSince                   15441 non-null  float64\n",
      " 7   Total Bags                  15441 non-null  float64\n",
      " 8   Small Bags                  15441 non-null  float64\n",
      " 9   Large Bags                  15441 non-null  float64\n",
      " 10  XLarge Bags                 15441 non-null  float64\n",
      " 11  type_conventional           15441 non-null  float64\n",
      " 12  type_organic                15441 non-null  float64\n",
      " 13  region_Albany               15441 non-null  float64\n",
      " 14  region_Atlanta              15441 non-null  float64\n",
      " 15  region_BaltimoreWashington  15441 non-null  float64\n",
      " 16  region_Boise                15441 non-null  float64\n",
      " 17  region_Boston               15441 non-null  float64\n",
      " 18  region_BuffaloRochester     15441 non-null  float64\n",
      " 19  region_California           15441 non-null  float64\n",
      " 20  region_Charlotte            15441 non-null  float64\n",
      " 21  region_Chicago              15441 non-null  float64\n",
      " 22  region_CincinnatiDayton     15441 non-null  float64\n",
      " 23  region_Columbus             15441 non-null  float64\n",
      " 24  region_DallasFtWorth        15441 non-null  float64\n",
      " 25  region_Denver               15441 non-null  float64\n",
      " 26  region_Detroit              15441 non-null  float64\n",
      " 27  region_GrandRapids          15441 non-null  float64\n",
      " 28  region_GreatLakes           15441 non-null  float64\n",
      " 29  region_HarrisburgScranton   15441 non-null  float64\n",
      " 30  region_HartfordSpringfield  15441 non-null  float64\n",
      " 31  region_Houston              15441 non-null  float64\n",
      " 32  region_Indianapolis         15441 non-null  float64\n",
      " 33  region_Jacksonville         15441 non-null  float64\n",
      " 34  region_LasVegas             15441 non-null  float64\n",
      " 35  region_LosAngeles           15441 non-null  float64\n",
      " 36  region_Louisville           15441 non-null  float64\n",
      " 37  region_MiamiFtLauderdale    15441 non-null  float64\n",
      " 38  region_Midsouth             15441 non-null  float64\n",
      " 39  region_Nashville            15441 non-null  float64\n",
      " 40  region_NewOrleansMobile     15441 non-null  float64\n",
      " 41  region_NewYork              15441 non-null  float64\n",
      " 42  region_Northeast            15441 non-null  float64\n",
      " 43  region_NorthernNewEngland   15441 non-null  float64\n",
      " 44  region_Orlando              15441 non-null  float64\n",
      " 45  region_Philadelphia         15441 non-null  float64\n",
      " 46  region_PhoenixTucson        15441 non-null  float64\n",
      " 47  region_Pittsburgh           15441 non-null  float64\n",
      " 48  region_Plains               15441 non-null  float64\n",
      " 49  region_Portland             15441 non-null  float64\n",
      " 50  region_RaleighGreensboro    15441 non-null  float64\n",
      " 51  region_RichmondNorfolk      15441 non-null  float64\n",
      " 52  region_Roanoke              15441 non-null  float64\n",
      " 53  region_Sacramento           15441 non-null  float64\n",
      " 54  region_SanDiego             15441 non-null  float64\n",
      " 55  region_SanFrancisco         15441 non-null  float64\n",
      " 56  region_Seattle              15441 non-null  float64\n",
      " 57  region_SouthCarolina        15441 non-null  float64\n",
      " 58  region_SouthCentral         15441 non-null  float64\n",
      " 59  region_Southeast            15441 non-null  float64\n",
      " 60  region_Spokane              15441 non-null  float64\n",
      " 61  region_StLouis              15441 non-null  float64\n",
      " 62  region_Syracuse             15441 non-null  float64\n",
      " 63  region_Tampa                15441 non-null  float64\n",
      " 64  region_TotalUS              15441 non-null  float64\n",
      " 65  region_West                 15441 non-null  float64\n",
      " 66  region_WestTexNewMexico     15441 non-null  float64\n",
      " 67  Month_April                 15441 non-null  float64\n",
      " 68  Month_August                15441 non-null  float64\n",
      " 69  Month_December              15441 non-null  float64\n",
      " 70  Month_February              15441 non-null  float64\n",
      " 71  Month_January               15441 non-null  float64\n",
      " 72  Month_July                  15441 non-null  float64\n",
      " 73  Month_June                  15441 non-null  float64\n",
      " 74  Month_March                 15441 non-null  float64\n",
      " 75  Month_May                   15441 non-null  float64\n",
      " 76  Month_November              15441 non-null  float64\n",
      " 77  Month_October               15441 non-null  float64\n",
      " 78  Month_September             15441 non-null  float64\n",
      "dtypes: float64(79)\n",
      "memory usage: 9.9 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")\n",
    "X_train_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.316943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.247612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_September</th>\n",
       "      <td>0.112086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_October</th>\n",
       "      <td>0.100772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.100081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.086358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_April</th>\n",
       "      <td>-0.087222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_February</th>\n",
       "      <td>-0.120834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_January</th>\n",
       "      <td>-0.179477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateSince</th>\n",
       "      <td>-0.230822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Coef\n",
       "AveragePrice         0.316943\n",
       "year                 0.247612\n",
       "Month_September      0.112086\n",
       "Month_October        0.100772\n",
       "region_SanFrancisco  0.100081\n",
       "...                       ...\n",
       "region_Houston      -0.086358\n",
       "Month_April         -0.087222\n",
       "Month_February      -0.120834\n",
       "Month_January       -0.179477\n",
       "DateSince           -0.230822\n",
       "\n",
       "[79 rows x 1 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge\n",
    "# lec19\n",
    "\n",
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, Ridge(alpha=1)) # i tried different alphas but they didn't affect train/test scores\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"ridge\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)\n",
    "\n",
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both using no `Date` and One-hot encoding `Month` gave good results of 0.80 on test set, with One-hot encoding of `Month` giving a slightly higher train score. Surprisingly, using `DateSince` combined with `Month` gave worse-than-baseline test score of 0.67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: very short answer questions\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "The following questions pertain to Lecture 19 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time series data tends to be irregular in scenarios such as the stock market, where sales and purchases occur at unpredictable rates. Consequently, it is difficult to predict when transactions will happen, leading to irregular time intervals between trades.\n",
    "\n",
    "2. Lagged versions of features would likely encounter more difficulties, as they assume that past values are from regularly spaced time points when making predictions. If the data points are irregularly spaced, the predictions could be greatly impacted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Survival analysis\n",
    "\n",
    "_Points:_ 6\n",
    "\n",
    "The following questions pertain to Lecture 20 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer?\n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The problem is that one could be churned after a long time and gets labelled churned, whereas a new customer who has not churned yet as of now will not be measured churned even if he might churn later spending less time than the churned customer. we don't just want to know the status of the customer after they have churned; it would be more useful to predict if a customer is about to churn or not so that actions can be taken to retain the customer.\n",
    "2. It depends but a customer who has just joined is more likely to churn, because if a customer has already been with a service for a year, they probably like the service. This can be seen in the relationships in the tenure column, with higher tenure being less likely to have a \"yes\" for churn.\n",
    "3. It means that the probability of the customer surviving or quitting the service is unchanging during that period which means the customer is very unlikely to leave during that period. The actual probability may be high or low, but the flat part of the curve means that it's not changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from â€œ1â€ will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using [PrairieLearn](https://ca.prairielearn.com/pl/course_instance/6697). Don't forget to rename your file `hw8_sol.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
